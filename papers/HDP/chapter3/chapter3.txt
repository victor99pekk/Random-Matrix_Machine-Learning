
Big Picture:
- We study random vectors X = (X1, ..., Xn) in R^n for very large n.
- High-dimensional spaces behave differently from low dimensions – leading to the "curse of dimensionality" but also offering powerful probabilistic tools.

Main Topics:

3.1 Concentration of the Norm
- Key result: If X has independent, mean-zero, unit-variance sub-gaussian coordinates, then ||X||_2 is very close to sqrt(n) with high probability.
- Uses Bernstein’s inequality for sub-exponential variables to show concentration.
- Takeaway: Random vectors concentrate around spheres of radius sqrt(n).

3.2 Covariance Matrices and Principal Component Analysis (PCA)
- Defines the covariance matrix: cov(X) = E[(X-μ)(X-μ)^T].
- Introduces isotropy: E[XX^T] = I_n.
- PCA: Find principal components (eigenvectors with largest eigenvalues) to capture most variance with fewer dimensions.
- Important: In isotropic distributions, projections <X,x> have variance ||x||_2^2.

3.3 Examples of High-Dimensional Distributions
- Spherical distributions: Uniformly on a sphere.
- Symmetric Bernoulli: Vectors with ±1 coordinates (uniform on {−1, 1}^n).
- Multivariate Gaussians: Standard normal N(0, I_n) is isotropic and rotation-invariant.
- Frames: Generalizations of orthonormal bases without requiring linear independence.
- Convex sets: Uniform distributions on isotropically transformed convex bodies.

3.4 Sub-gaussian Distributions in Higher Dimensions
- Defines sub-gaussian random vectors: all projections <X,x> are sub-gaussian.
- If X has independent sub-gaussian coordinates, then X is sub-gaussian.
- Uniform distribution on spheres is sub-gaussian.
- Discrete distributions need exponentially many points to behave sub-gaussian.

3.5 Application: Grothendieck’s Inequality
- Grothendieck’s inequality connects bounded bilinear forms over {±1} variables to inner products over arbitrary unit vectors.
- Proof uses a probabilistic method with Gaussian random vectors.
- Important for approximations in optimization (e.g., semidefinite programming).

Key Concepts to Remember:
- Concentration phenomena (e.g., norms near sqrt(n)).
- Isotropic vectors and their properties.
- Sub-gaussian behavior and why it matters.
- Geometry of random high-dimensional vectors (almost orthogonality, spherical shells).
- Applications to optimization (Grothendieck’s inequality).
